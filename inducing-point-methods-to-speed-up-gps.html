<!DOCTYPE html>
<html lang="en">
  <head>
  

    <meta name="tags" content="gp" />
    <meta name="tags" content="gsoc" />
    <meta name="tags" content="gp-approximations" />

    <title>Inducing point methods to speed up GPs - posts</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet" />
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://bwengals.github.io/theme/style.css" rel="stylesheet" />
    <link href="https://bwengals.github.io/theme/notebooks.css" rel="stylesheet" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body id="index" class="archive">
    <!--[if lt IE 7]>
        <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
    <![endif]-->
    <nav class="navbar navbar-default" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="https://bwengals.github.io">Bill Engels</a>
        </div>
        <div class="collapse navbar-collapse navbar-right">
          <ul class="nav navbar-nav">
            <li><a href="https://bwengals.github.io/tags.html">tags</a></li>
          </ul>
        </div>
        <!-- /.navbar-collapse -->
      </div>
    </nav>
    <div class="container">
    <section id="content" class="body">
      <header>
        <h1 class="entry-title">
          Inducing point methods to speed up GPs
        </h1>
        
        <div class="text-muted">Thu 01 June 2017</div>
      </header>
<!-- .entry-content -->
      <div class="article_content">
        <p>Another main avenue for speeding up GPs is inducing point methods, or sparse GPs.</p>
<!-- PELICAN_END_SUMMARY -->

<p>The idea is to reduce the effective number of input data points <span class="math">\(x\)</span> to the GP from <span class="math">\(n\)</span> to <span class="math">\(m\)</span>, with <span class="math">\(m &lt; n\)</span>, where the set of <span class="math">\(m\)</span> points are called <strong>inducing points</strong>.  Since this makes the effective covariance matrix <span class="math">\(K\)</span> smaller, many inducing point approaches reduce the computational complexity from <span class="math">\(\mathcal{O}(n^3)\)</span> to <span class="math">\(\mathcal{O}(nm^2)\)</span>.  The smaller <span class="math">\(m\)</span> is, the bigger the speed up.  </p>
<p>Many algorithms have been proposed to actually select the location of the <span class="math">\(m\)</span> inducing points.  The simplest approach is to select some subset of the <span class="math">\(x\)</span> points.  Doing this selection in a "smart" way is a difficult combinatorical optimization problem since the choices are discrete.  Another option is to choose <span class="math">\(m\)</span> points that aren't necessarily in the set of <span class="math">\(x\)</span> points but lie on the <span class="math">\(x\)</span> domain.  This is an easier, continuous, optimization problem.  The need to choose inducing points is common to all the GP approximation approaches.  Beyond the choice of inducing point locations, these methods then approximate either the GP prior, likelihood or both.  Sometimes this approximation is expressed as a modification of the GP prior covariance.  This post is for the most part a summary of <a href="http://www.jmlr.org/papers/v6/quinonero-candela05a.html">Quinonero-Candela &amp; Rasmussen</a> and <a href="https://arxiv.org/abs/1503.01057">Wilson &amp; Nickisch</a>.</p>
<h1>2.  Inducing point methods</h1>
<p>The inducing points of the approximate GP can be viewed as a set of <span class="math">\(m\)</span> latent variables denoted <span class="math">\(u\)</span>.  They are the values of the GP at the inducing input locations <span class="math">\(z\)</span>.  The <a href="https://bwengals.github.io/gp-speedups-1.html#Inferring-the-posterior-distribution-of-the-hyperparameters">GP prior distribution</a> (given hyperparameter values) is <span class="math">\(p(f \mid \phi \,, \theta)\)</span>.  In our notation, <span class="math">\(k(x, z)\)</span> is a covariance <strong>function</strong> to be evaluated across <span class="math">\(x\)</span> and <span class="math">\(z\)</span>, and <span class="math">\(K_{x,z}\)</span> is a covariance <strong>matrix</strong> that has been evaluated across <span class="math">\(x\)</span> and <span class="math">\(z\)</span>.  The joint GP prior on <span class="math">\(f\)</span> and the prediction points <span class="math">\(f_*\)</span> is</p>
<div class="math">$$
p(f, f_*) = N\left( 0 \,, \begin{bmatrix}
                            K_{x,x}   &amp; K_{x, x_*} \\                            
                            K_{x_*, x} &amp; K_{x_*, x_*} \\
                          \end{bmatrix} \right) \,.
$$</div>
<p>This is just the regular GP prior, where the new <span class="math">\(x\)</span> points, <span class="math">\(x_*\)</span> are included as a joint distribution.  We assume a zero mean function <span class="math">\(\mu = 0\)</span>, and neglect <span class="math">\(\phi\)</span> and <span class="math">\(\theta\)</span> from the notation.  The inducing points are are another set of latent variables, so the GP prior can be recovered by marginalizing out the inducing points</p>
<div class="math">$$
p(f, f_*) = \int p(f, f_*, u) \, du = \int p(f, f_* \mid u) p(u) \, du \,,
$$</div>
<p>where <span class="math">\(p(u) = N(0\,, k(z, z'))\)</span>, where the covariance matrix obtained by evaluating <span class="math">\(k(z, z')\)</span> is <span class="math">\(m \times m\)</span>.  <a href="http://www.jmlr.org/papers/v6/quinonero-candela05a.html">Quinonero-Candela &amp; Rasmussen</a> show that the majority of sparse GP approximations proposed in the literature approximate the GP prior <span class="math">\(p(f, f_*)\)</span> by assuming that <strong><span class="math">\(f\)</span> and <span class="math">\(f_*\)</span> are conditionally independent given the inducing points <span class="math">\(u\)</span></strong>,</p>
<div class="math">$$
p(f, f_*) \approx q(f, f_*) = \int q(f_* \mid u) q(f \mid u) p(u) \, du \,.
$$</div>
<ul>
<li><span class="math">\(q(f \mid u)\)</span> is the <strong>training conditional</strong> distribution</li>
<li><span class="math">\(q(f_* \mid u)\)</span> is the <strong>test conditional</strong> distribution</li>
</ul>
<p>It is also useful to define</p>
<div class="math">$$
Q_{a,b} = K_{a, z} K_{z,z}^{-1} K_{z, b} \,.
$$</div>
<p><span class="math">\(f\)</span> influences <span class="math">\(f_*\)</span> through <span class="math">\(u\)</span>, which is why <span class="math">\(u\)</span> are called inducing variables.  <span class="math">\(u\)</span> is the value of the GP evaluated at <span class="math">\(z\)</span>.</p>
<h2>SoR</h2>
<p>The subset of regressors (SoR) approximation is the simplest, but it is not usually recommended for use in practice since its predictive uncertainties (not discussed here) are off the mark.  The approximations it proposes to use for the training and test conditional are</p>
<ul>
<li><span class="math">\(q(f, u) = N(K_{f,z} K_{z,z}^{-1} u\,, 0)\)</span></li>
<li><span class="math">\(q(f_*, u) = N(K_{*,z} K_{z,z}^{-1} u\,, 0)\)</span></li>
</ul>
<p>SoR imposes a deteriminstic relation between <span class="math">\(f\)</span> and <span class="math">\(u\)</span>, since the covariance is zero.  SoR can also be viewed as having the effective covariance matrix</p>
<div class="math">$$
\tilde{K}^{SoR} = K_{x,z} K_{z,z}^{-1} K_{z,x}  
$$</div>
<p>which is not of full rank (<a href="https://arxiv.org/abs/1503.01057">KISSGP</a>).</p>
<h2>DTC</h2>
<p>The deterministic training conditional (DTC) approximation doesn't suffer from the problems with the predictive uncertainties, but has the same predictive mean.  It has the same <span class="math">\(p(f, u)\)</span> as SoR,</p>
<ul>
<li><span class="math">\(q(f, u) = N(K_{f,z} K_{z,z}^{-1} u\,, 0)\)</span></li>
<li><span class="math">\(q(f_*, u) = p(f_*, u)\)</span></li>
</ul>
<p>and uses <span class="math">\(p(f_*, u)\)</span>.</p>
<h2>FITC</h2>
<p>Fully independent training conditional (FITC) is the sparse GP approach implemented in <a href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a>.</p>
<ul>
<li><span class="math">\(q(f \mid u) = \prod_{i=1}^{n} = N(K_{f,z} K_{z,z}^{-1}, diag[K_{f,f} - Q_{f,f}])\)</span></li>
<li><span class="math">\(q(f_* \mid u) = p(f_* \mid u)\)</span></li>
</ul>
<p>The relationship between <span class="math">\(f\)</span> and <span class="math">\(u\)</span> is not deterministic in this case.  The covariance is nonzero.  The effective covariance matrix for FITC is</p>
<div class="math">$$
\tilde{K}^{FITC} = \tilde{K}^{SoR} + \delta_{xz}( K_{x,x'} - \tilde{K}^{SoR}) \,.
$$</div>
<p>It is full rank (DTC does not) due to diagonal correction to the covariance matrix.</p>
<h2>SKI (Structured Kernel Interpolation)</h2>
<p>is described in the KISS-GP paper.  I'm most interested in this method right now, since it is a generalization of inducing point methods such as SoR and FITC.  It is also implemented in GPML. </p>
<p>It approximates the true covariance matrix by <em>interpolating an <span class="math">\(m \times m\)</span> covariance matrix</em>,</p>
<div class="math">$$
K_{x, z} \approx W K_{z,z} \,.
$$</div>
<p>Depending on the interpolation scheme used, <span class="math">\(W\)</span> can be very sparse.  For example, if <span class="math">\(W\)</span> represents local cubic interpolation, each row of <span class="math">\(W\)</span> will have 4 non-zero entries.  Since <span class="math">\(W\)</span> is sparse, matrix multiplications can use faster, sparse linear algebra routines.  Since we are interpolating <span class="math">\(K_{z,z}\)</span>, we are free to choose inducing points that can <a href="https://bwengals.github.io/gp-speedups-2.html">exploit matrix structure</a> such that <span class="math">\(K_{u,u}\)</span> has Toeplitz and/or Kronecker structure.  The true covariance matrix <span class="math">\(K_{x,x}\)</span> is approximated as</p>
<div class="math">$$
K_{x,x} \approx W K_{z,z} W^T
$$</div>
<p>where <span class="math">\(W\)</span> is the matrix of interpolation weights.  The computational complexity of this approach depends on the number of inducing points used and how they are structured.  It isn't strictly necessary for the inducing points to be regularly spaced or on a multidimensional grid.  However, placing them on in a grid structure allows for a large number of inducing points to be used (even <span class="math">\(m &gt; n\)</span>), resulting in accurate interpolations.  The authors refer to SKI+Toeplitz+Kronecker as KISS-GP. It was recently implemented in the GPML version 4.0 MATLAB package.</p>
<h3>References</h3>
<ul>
<li><a href="http://www.jmlr.org/papers/v6/quinonero-candela05a.html">Quinonero-Candela &amp; Rasmussen</a></li>
<li><a href="https://arxiv.org/abs/1503.01057">KISS-GP</a></li>
<li><a href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML V4.0</a></li>
<li><a href="https://arxiv.org/abs/1511.01870">Wilson et al. 2015</a></li>
</ul>
<h2>Also</h2>
<p><a href="https://github.com/GPflow/GPflow">GPflow</a> uses a sparse GP model distinct from those mentioned above.  It uses a variational approach to infer both the inducing points and the covariance function parameters.  I plan on researching their approach further.  See <a href="http://gpflow.readthedocs.io/en/latest/notebooks/SGPR_notes.html">here</a> for a rundown. </p>
<h3>Conclusion</h3>
<p>Inducing point methods are attractive because they can largely be treated in a black box fashion.  However, as <span class="math">\(m\)</span> decreases, the quality of the approximation decreases.  A GP can be viewed as a model that fits a function using an infinite set of basis functions, which gives it more representation power than models with a fixed set of basis functions.  Unfortunately, using a smaller number of inducing points reduces the expressiveness of GPs.  One reason random forest type models or deep learning approaches have been successful on large datasets is their large number of parameters support expressive models.  It's a catch-22 that large data sets which would benefit the most from GP models necessitate approximations that degrade the representation power of GPs.  </p>
<p>At this point, the SKI approach where inducing points are placed on a grid, is the most promising approach to me.<br />
The GPML package implements most of the GP speed up methods discussed in the past few posts.  My next step will be to run timing tests using the GPML package to get a feel for the speedups and approximation quality offered by SKI vs FITC, and how much the Toeplitz or Kronecker structure in the inducing points contribute.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
      </div>
<!-- /.entry-content -->
      <footer class="post-info text-muted">
        <button type="button" class="btn btn-default">          
          <a href="https://bwengals.github.io/category/posts.html"><div class="fa fa-lg fa-folder-open"></div> posts</a>
        </button>
        <button type="button" class="btn btn-default">
          <a href="https://bwengals.github.io/tag/gp.html"><div class="fa fa-lg fa-tag"></div> gp</a>
        </button>
        <button type="button" class="btn btn-default">
          <a href="https://bwengals.github.io/tag/gsoc.html"><div class="fa fa-lg fa-tag"></div> gsoc</a>
        </button>
        <button type="button" class="btn btn-default">
          <a href="https://bwengals.github.io/tag/gp-approximations.html"><div class="fa fa-lg fa-tag"></div> gp-approximations</a>
        </button>
      </footer>
      <!-- /.post-info -->
    </section>
    </div>
    <footer class="footer">
      <div class="container">
        <p class="footer-text">&copy; <a href="https://bwengals.github.io">Bill Engels</a> powered by <a href="http://getpelican.com/">pelican</a></p>
      </div>
    </footer>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  </body>
</html>